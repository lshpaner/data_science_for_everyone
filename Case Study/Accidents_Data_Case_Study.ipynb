{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NIOSH - Data Science for Everyone Workshop - Accidents Data Case Study (Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Leonid Shpaner for use in **NIOSH - Data Science for Everyone Workshop.** The dataset\n",
    "originates for Data Mining from Business Analytics (Shmueli et., 2018). The functions and syntax are presented in the most basic format to facilitate ease of use.  \n",
    "\n",
    "The Accidents dataset is presented as a flat `.csv` file which is comprised of 42,183 recorded automobile\n",
    "accidents from 2001 in the United States. The following three outcomes are observed: “NO INJURY,\n",
    "INJURY, or FATALITY.” Each accident is supplemented with additional information (i.e., day of the week,\n",
    "condition of weather, and road type). This may be of interest to an organization looking to develop “a\n",
    "system for quickly classifying the severity of an accident based on initial reports and associated data in the\n",
    "system (some of which rely on GPS-assisted reporting)” (Shmueli et al., 2018, p. 202)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading, Pre-Processing, and Exploring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the dataset is in the same path as our Python script. If you save the\n",
    "data somewhere else, you need to pass in the full path to where you saved the dataset,  e.g. `dataset = pd.read_csv('C:/Downloads/dataset.csv')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's install the necessary libraries first, uncommenting (removing the `#` symbol) in front of the commands in the cell blocks, and then running them, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas; pip install statsmodels; pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pydotplus; pip install prettytable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load these necessary libraries as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from prettytable import PrettyTable \n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc, mean_squared_error,\\\n",
    "precision_score, recall_score, f1_score, accuracy_score,\\\n",
    "confusion_matrix, plot_confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we proceed to read in the flat `.csv` file, and examine the first 4 rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/lshpaner/data_science_\\\n",
    "for_everyone/main/Case%20Study/accidentsFull.csv'\n",
    "accidents = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    "Prior to delving deeper, let us first identify (describe) what each respective variable name really means. To\n",
    "this end, we have the following data dictionary:  \n",
    "\n",
    "1. **HOUR_I_R** - rush hour classification: 1 = rush hour, 0 = not rush hour (rush hour = 6-9 am, or 4-7\n",
    "pm)  \n",
    "2. **ALCHL_I** - alcohol involvement: Alcohol involved = 1, alcohol not involved = 2  \n",
    "3. **ALIGN_I** - road alignment: 1 = straight, 2 = curve  \n",
    "4. **STRATUM_R** - National Automotive Sampling System stratum: 1 = NASS Crashes involving at least one\n",
    "passenger vehicle (i.e., a passenger car, sport utility Vehicle, pickup truck or van) towed due to damage\n",
    "from the crash scene and no medium or heavy trucks are involved. 0 = not  \n",
    "5. **WRK_ZONE** - work zone: 1= yes, 0 = no  \n",
    "6. **WKDY_I_R** - weekday or weekend: 1 = weekday, 0 = weekend  \n",
    "7. **INT_HWY** - interstate highway: 1 =yes, 0 = no  \n",
    "8. **LGTCON_I_R** - light conditions - 1=day, 2=dark (including dawn/dusk), 3 = dark, but lighted, 4 = dawn\n",
    "or dusk  \n",
    "9. **MANCOL_I** - type of collision: 0 = no collision, 1 = head-on, 2 = other form of collision  \n",
    "10. **PED_ACC_R** - collision involvement type: 1=pedestrian/cyclist involved, 0=not  \n",
    "11. **RELJCT_I_R** - whether the collision occurred at intersection: 1=accident at intersection/interchange,\n",
    "0=not at intersection  \n",
    "12. **REL_RWY_R** - related to roadway or not: 1 = accident on roadway, 0 = not on roadway  \n",
    "13. **PROFIL_I_R** -  road profile: 1 = level, 0 = other  \n",
    "14. **SPD_LIM** - speed limit, miles per hour: numeric  \n",
    "15. **SUR_CON** - surface conditions (1 = dry, 2 = wet, 3 = snow/slush, 4 = ice, 5 = sand/dirt/oil, 8 = other,\n",
    "9 = unknown)  \n",
    "16. **TRAF_CON_R** - traffic control device: 0 = none, 1 = signal, 2 = other (sign, officer, . . . )  \n",
    "17. **TRAF_WAY** - traffic type: 1 = two-way traffic, 2 = divided hwy, 3 = one-way road  \n",
    "18. **VEH_INVL** - vehicle involvement: number of vehicles involved (numeric)  \n",
    "19. **WEATHER_R** - weather conditions: 1=no adverse conditions, 2=rain, snow or other adverse condition  \n",
    "20. **INJURY_CRASH** - injury crash: 1 = yes, 0 = no  \n",
    "21. **NO_INJ_I** - number of injuries: numeric  \n",
    "22. **PRPTYDMG_CRASH** - property damage: 1 = property damage, 2 = no property damage  \n",
    "23. **FATALITIES** - fatalities: 1 = yes, 0 = no  \n",
    "24. **MAX_SEV_IR** - maximum severity: 0 = no injury, 1 = non-fatal injury, 2 = fatal injury  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Pre-Processing Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speed limit (`SPD_LIM`) has valuable numerical information, so let us go ahead and create buckets for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_speed = accidents['SPD_LIM'].unique()\n",
    "unique_speed.sort()\n",
    "unique_speed = pd.DataFrame(unique_speed)\n",
    "unique_speed.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [ \"{0} - {1}\".format(i, i + 5) for i in range(0, 100, 10) ]\n",
    "accidents['MPH Range'] = pd.cut(accidents.SPD_LIM, range(0, 105, 10), \n",
    "                                right=False, \n",
    "                                labels=labels)\n",
    "# inspect the new dataframe with this info\n",
    "accidents[['SPD_LIM', 'MPH Range']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents['MPH Range'] = accidents['SPD_LIM'].map({5:'5-10', 10:'5-10', \n",
    "                                                   15: '15-20', 20:'15-20',\n",
    "                                                   25: '25-30', 30: '25-30',\n",
    "                                                   35: '35-40', 40: '35-40',\n",
    "                                                   45: '45-50', 50: '45-50',\n",
    "                                                   55: '55-60', 60: '55-60',\n",
    "                                                   65: '65-70', 70: '65-70',\n",
    "                                                   75: '75'})\n",
    "accidents['MPH Range']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a dummy variable called `INJURY` to determine if the accident resulted in an injury based on maximum severity. So, if the severity of the injury is greater than zero, we specify `yes`. Otherwise, we specify `no`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents['INJURY'] = np.where(accidents['MAX_SEV_IR'] > 0, 'yes', 'no')\n",
    "accidents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first examine the structure of this dataset so we can gather the details about the size, shape, and values of the dataframe holistically, and each column, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of Rows:', accidents.shape[0])\n",
    "print('Number of Columns:', accidents.shape[1], '\\n')\n",
    "\n",
    "data_types = accidents.dtypes\n",
    "data_types = pd.DataFrame(data_types)\n",
    "data_types = data_types.assign(Null_Values = \n",
    "                               accidents.isnull().sum())\n",
    "data_types.reset_index(inplace = True)\n",
    "data_types.rename(columns={0:'Data Type',\n",
    "                          'index': 'Column/Variable',\n",
    "                          'Null_Values': \"# of Nulls\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many accidents resulted in injuries? We create a stylistic pandoc table from the `PrettyTable()` library to\n",
    "inspect these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injury_yes = accidents['INJURY'].value_counts()['yes']\n",
    "injury_no = accidents['INJURY'].value_counts()['no']\n",
    "injury_total = injury_yes + injury_no\n",
    "\n",
    "table1 = PrettyTable() # build a table\n",
    "table1.field_names = ['Yes: Injured', 'No: Uninjured', \n",
    "                      'Total Injured']\n",
    "table1.add_row([injury_yes, injury_no, injury_total])\n",
    "print(table1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What percentage of accidents resulted in injuries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_inj = injury_yes/(injury_yes + injury_no) \n",
    "print(round(perc_inj, 2)*100, '% of accidents'\n",
    "                       ' resulted in injuries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little more than half of the accidents resulted in injuries; thus, we should intrinsically focus our predictions in favor of injuries. However, predictive analytics requires more than merely a cursory glance at first tier probability results. Therefore, we cannot make any assumptions at face value. We will proceed to model this behavior later, but for now let us continue exploring the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accidents injury bar graph\n",
    "injury_count = accidents['INJURY'].value_counts()\n",
    "fig = plt.figure(figsize=(3,2))\n",
    "injury_count.plot.bar(x ='lab', y='val', rot=0, width=0.99, \n",
    "                         color=\"steelblue\")\n",
    "plt.title ('Bar Graph of Accidents Resulting in Injury')\n",
    "plt.xlabel('Injury') \n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Injury Outcomes by Miles per Hour'+\"\\033[1m\")\n",
    "\n",
    "def INJURY_by_MPH():\n",
    "\n",
    "    INJURY_yes = accidents.loc[accidents.INJURY == 'yes'].groupby(\n",
    "                               ['MPH Range'])[['INJURY']].count()\n",
    "    \n",
    "    INJURY_yes.rename(columns = {'INJURY':'Yes'}, inplace=True)\n",
    "\n",
    "    INJURY_no = accidents.loc[accidents.INJURY == 'no'].groupby(\n",
    "                               ['MPH Range'])[['INJURY']].count()\n",
    "    \n",
    "    INJURY_no.rename(columns = {'INJURY':'No'}, inplace=True)\n",
    "\n",
    "    INJURY_comb = pd.concat([INJURY_yes, INJURY_no], axis = 1)\n",
    "\n",
    "    # sum row totals\n",
    "    INJURY_comb['Total'] = INJURY_comb.sum(axis=1)\n",
    "    INJURY_comb.loc['Total'] = INJURY_comb.sum(numeric_only = True, \n",
    "                                               axis=0)\n",
    "    # get % total of each row\n",
    "    INJURY_comb['% Injured'] = round((INJURY_comb['Yes'] / \n",
    "                                      (INJURY_comb['Yes'] \n",
    "                                     + INJURY_comb['No']))* 100, 2)\n",
    "    \n",
    "    return INJURY_comb.style.format(\"{:,.0f}\")\n",
    "\n",
    "INJURY_by_MPH()\n",
    "mph_inj = INJURY_by_MPH().data # retrieve dataframe\n",
    "mph_inj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mph_plt = mph_inj['Total'][0:8].sort_values(ascending=False)\n",
    "mph_plt.plot(kind='bar', width=0.90)\n",
    "plt.title('Accidents by Speed Limit')\n",
    "plt.xlabel('Speed Limit (in MPH)')\n",
    "plt.ylabel('# of Accidents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note.*** The 35-40 mph speed limit shows the highest prevalence of accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig.tight_layout(pad=6)\n",
    "\n",
    "mph_plt2 = mph_inj[['Yes', 'No']][0:8].sort_values(by=['Yes'], \n",
    "                                                   ascending=False)\n",
    "mph_plt2.plot(kind='bar', stacked=True, \n",
    "              ax=ax1, color = ['#F8766D', '#00BFC4'], width = 0.90)\n",
    "ax1.set_title('Speed Limit by Injury Outcome: (Injured or Not)')\n",
    "ax1.set_xlabel('Speed Limit (in Miles per Hour)')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "# normalize the plot and plot it\n",
    "mph_plt_norm = mph_plt2.div(mph_plt2.sum(1), axis = 0)\n",
    "mph_plt_norm.plot(kind='bar', stacked=True, \n",
    "                  ax=ax2,color = ['#F8766D', '#00BFC4'], width = 0.90)\n",
    "ax2.set_title('Speed Limit by Injury Outcome: (Injured or Not): Normalized')\n",
    "ax2.set_xlabel('Speed Limit (in Miles per Hour)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the speed limit group bar graph overlayed with “injured” and “non-injured” accident results, it is\n",
    "evident that the speed limit of 35-40 mph has a greater incidence of injuries (more than any other speed\n",
    "limit group).  \n",
    "\n",
    "While the strength of this graph is in its depiction of the overall distribution (providing us with injuries\n",
    "vs. non-injuries in each speed related accident), it does little to provide a comparison of the frequency\n",
    "(incidence rate) of injuries among the speed limit groups.\n",
    "\n",
    "Normalizing the speed limit groups by our target (INJURY) assuages this analysis in such capacity. From\n",
    "here, it is easier to see that speed limits of 5-10 miles per hour, and 35-40 miles per hour, respectively had\n",
    "roughly 50% injury rates, whereas (notably), 75 miles per hour exhibited the highest injury rate of all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note.*** There are precisely 12,845 accidents that occurred between the 35-40 mph speed limit. 6,873 (or\n",
    "53.51%) of them resulted in injuries.\n",
    "Now, let us plot the histogram distributions from each respective variable of the dataset. Figure 3 below visually illustrates these distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for degenerate distributions\n",
    "accidents.hist(grid=False, figsize=(16,15))\n",
    "                            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23 out of the original 24 variables are categorical, and from the histograms presented herein it is possible\n",
    "to uncover degenerate distributions with relative ease, as one category represents higher values over another.\n",
    "However, we look to the speed limit as the sole quantitative predictor which yields a positively skewed distribution. The following summary statistics corroborate this claim since the mean of 44 is greater than\n",
    "the median of 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_stats = pd.DataFrame(accidents['SPD_LIM'].describe()).T\n",
    "summ_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Accidents by Speed Limit Summary'+\"\\033[1m\")\n",
    "def accident_stats_by_mph():\n",
    "    pd.options.display.float_format = '{:,.2f}'.format\n",
    "    new2 = accidents.groupby('MPH Range')['SPD_LIM']\\\n",
    "    .agg([\"mean\", \"median\", \"std\", \"min\", \"max\"])\n",
    "    new2.loc['Total'] = new2.sum(numeric_only=True, axis=0)\n",
    "    column_rename = {'mean': 'Mean', 'median': 'Median',\n",
    "                     'std': 'Standard Deviation',\\\n",
    "                     'min':'Minimum','max': 'Maximum'}\n",
    "    dfsummary = new2.rename(columns = column_rename)\n",
    "    return dfsummary\n",
    "acc_stats_mph = accident_stats_by_mph()\n",
    "accident_stats_by_mph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_stats_mph.iloc[:, 0:3][0:8].plot.barh(figsize=(8,3.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Boxplot Distribution - Speed Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected boxplot distributions\n",
    "print(\"\\033[1m\"+'Boxplot Distribution'+\"\\033[1m\")\n",
    "\n",
    "# Boxplot of age as one way of showing distribution\n",
    "fig = plt.figure(figsize = (10,1.5))\n",
    "plt.title ('Boxplot: Speed Limit')\n",
    "plt.xlabel('Speed Limit')\n",
    "plt.ylabel('Value')\n",
    "sns.boxplot(data=accidents['SPD_LIM'], \n",
    "            palette=\"coolwarm\", orient='h', \n",
    "            linewidth=2.5)\n",
    "plt.show()\n",
    "\n",
    "IQR = summ_stats['75%'][0] - summ_stats['25%'][0]\n",
    "\n",
    "print('The first quartile is %s. '%summ_stats['25%'][0])\n",
    "print('The third quartile is %s. '%summ_stats['75%'][0])\n",
    "print('The IQR is %s.'%round(IQR,2))\n",
    "print('The mean is %s. '%round(summ_stats['mean'][0],2))\n",
    "print('The standard deviation is %s. '%round(summ_stats['std'][0],2))\n",
    "print('The median is %s. '%round(summ_stats['50%'][0],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Whereas no outliers are present in the speed limit variable, there exists some skewness where the mean (43.55) is slightly greater than the median (40.00). Whereas typically a Box-Cox transformation could mitigate against skewness by transforming the variable(s) of interest, we will not be making such transformation to avoid misrepresenting the speed limit variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix title\n",
    "print(\"\\033[1m\"+'Accidents Data: Correlation Matrix'+\"\\033[1m\")\n",
    "\n",
    "# assign correlation function to new variable\n",
    "corr = accidents.corr()\n",
    "matrix = np.triu(corr) # for triangular matrix\n",
    "plt.figure(figsize=(20,20))\n",
    "# parse corr variable intro triangular matrix\n",
    "sns.heatmap(accidents.corr(method='pearson'), \n",
    "            annot=True, linewidths=.5, \n",
    "            cmap=\"coolwarm\", mask=matrix,\n",
    "            square = True, \n",
    "            cbar_kws={'label': 'Correlation Index'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us narrow our focus by removing highly correlated predictors and passing the rest into a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_matrix = accidents.corr().abs()\n",
    "upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),\n",
    "                                     k=1).astype(np.bool))\n",
    "\n",
    "to_drop = [column for column in upper_tri.columns if \n",
    "           any(upper_tri[column] > 0.75)]\n",
    "print('These are the columns we should drop: %s'%to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_1 = accidents.drop(columns=['MPH Range','NO_INJ_I','INJURY', \n",
    "                                      'INJURY_CRASH', 'MANCOL_I_R', \n",
    "                                      'FATALITIES', 'INT_HWY', 'TRAF_WAY'])\n",
    "accidents_1 = accidents_1.drop(to_drop, axis=1)\n",
    "print(accidents_1.dtypes, '\\n')\n",
    "print('Number of Rows:', accidents_1.shape[0])\n",
    "print('Number of Columns:', accidents_1.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MPH_Range` was created strictly for exploratory data analysis purposes. The `INJURY` column was based off the maximum injury severity column `MAX_SEV_IR,` so, we will binarize the `INJURY` column into a new `Injured` column in lieu of the prior two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents['Injured'] = accidents['INJURY'].map({'yes':1, 'no':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we must remove the `REL_RWY_R,` `PRPTYDMG_CRASH,` and `MAX_SEV_IR` columns from the dataframe resulting from the inherent between-predictor and predictor-target relationships, respectively. However, there are still a few predictors that warrant subsequent omission. Number of injuries (`NO_INJ_I`) and fatalities (`FATALATIES`) are inherently and intrinsically related to the outcome by virtue of their meaning. Therefore, in order to avoid overfitting the model, we remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_1 = accidents.drop(columns=['MPH Range','NO_INJ_I','INJURY',\n",
    "                                      'INJURY_CRASH', 'MANCOL_I_R',\n",
    "                                      'FATALITIES'])\n",
    "accidents_1 = accidents_1.drop(to_drop, axis=1)\n",
    "print(accidents_1.dtypes, '\\n')\n",
    "print('Number of Rows:', accidents_1.shape[0])\n",
    "print('Number of Columns:', accidents_1.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Statistical Significance Via Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model is introduced as a baseline because establishing impact of coefficients on each independent feature can be carried with relative ease. Moreover, it is possible to gauge statistical significance from the reported *p*-values of the summary output table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generalized Linear Model - Logistic Regression Baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = \\beta_0 + \\beta_1x_1 +\\beta_2x_2 +\\cdots+\\beta_px_p + \\varepsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression - Parametric Form**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p(y) = \\frac{\\text{exp}(\\beta_0+\\beta_1x_1+\\beta_2x_2+\\cdot\\cdot\\cdot+\\beta_px_p)}{1+\\text{exp}(\\beta_0+\\beta_1x_1+\\beta_2x_2+\\cdot\\cdot\\cdot+\\beta_px_p)}  + \\varepsilon $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression - Descriptive Form**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{p}(y) = \\frac{\\text{exp}(b_0+b_1x_1+b_2x_2+\\cdot\\cdot\\cdot+b_px_p)}{1+\\text{exp}(b_0+b_1x_1+b_2x_2+\\cdot\\cdot\\cdot+b_px_p)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = accidents_1.drop(columns=['Injured'])\n",
    "X = sm.add_constant(X)\n",
    "y = pd.DataFrame(accidents_1[['Injured']])\n",
    "log_results = sm.Logit(y,X, random_state=222).fit()\n",
    "log_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary output table, we observe that `WRK_ZONE`, `INT_HWY,` `LGTCON_I_R,` and `TRAF_WAY` have *p*-values of 0.168, 0.173, and 0.757, respectively, thereby ruling out statistical significance where α = 0.05. We will thus remove them from the refined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents1 = accidents_1.drop(columns=['WRK_ZONE','INT_HWY',\n",
    "                          'LGTCON_I_R', 'TRAF_WAY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_Test_Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = accidents_1.drop(columns=['Injured'])\n",
    "y = pd.DataFrame(accidents_1['Injured'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                   test_size=0.20, random_state=42)\n",
    "\n",
    "# confirming train_test_split proportions\n",
    "print('training size:', round(len(X_train)/len(X),2))\n",
    "print('test size:', round(len(X_test)/len(X),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm dimensions (size of newly partioned data)\n",
    "print('Training:', len(X_train))\n",
    "print('Test:', len(X_test))\n",
    "print('Total:',   len(X_train)\n",
    "                + len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Un-Tuned Logistic Regression Model\n",
    "logit_reg = LogisticRegression(random_state=222) \n",
    "logit_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "logit_reg_pred1 = logit_reg.predict(X_test)\n",
    "\n",
    "# accuracy and classification report\n",
    "print('Untuned Logistic Regression Model')\n",
    "print('Accuracy Score')\n",
    "print(accuracy_score(y_test, logit_reg_pred1))\n",
    "print('Classification Report \\n', \n",
    "       classification_report(y_test, logit_reg_pred1))\n",
    "\n",
    "# Tuned Logistic Regression Model\n",
    "C = [0.01, 0.1, 0.5, 1, 5, 10, 50]\n",
    "LRtrainAcc = []\n",
    "LRtestAcc = []\n",
    "for param in C:\n",
    "    tuned_lr = LogisticRegression(solver = 'saga',\n",
    "                                  C = param, \n",
    "                                  max_iter = 200,\n",
    "                                  n_jobs = -1,\n",
    "                                  random_state = 222)\n",
    "    tuned_lr.fit(X_train, y_train)\n",
    "    # Predict on train set\n",
    "    tuned_lr_pred_train = tuned_lr.predict(X_train)\n",
    "    # Predict on test set\n",
    "    tuned_lr1 = tuned_lr.predict(X_test)\n",
    "    LRtrainAcc.append(accuracy_score(y_train, tuned_lr_pred_train))\n",
    "    LRtestAcc.append(accuracy_score(y_test, tuned_lr1))\n",
    "    \n",
    "# accuracy and classification report\n",
    "print('Tuned Logistic Regression Model')\n",
    "print('Accuracy Score')\n",
    "print(accuracy_score(y_test, tuned_lr1))\n",
    "print('Classification Report \\n', \n",
    "       classification_report(y_test, tuned_lr1))\n",
    "\n",
    "# plot cost by accuracy\n",
    "fig, ax = plt.subplots(figsize=(6,2.5))\n",
    "ax.plot(C, LRtrainAcc, 'ro-', C, LRtestAcc,'bv--')\n",
    "ax.legend(['Training Accuracy','Test Accuracy'])\n",
    "plt.title('Logistic Regression: Accuracy vs. Cost')\n",
    "ax.set_xlabel('Cost'); ax.set_xscale('log')\n",
    "ax.set_ylabel('Accuracy'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we print the classification report for the un-tuned decision tree, let us establish the optimal maximum depth hyperparameter by varying it in a for-loop as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary the decision tree depth in a loop, \n",
    "# increasing depth from 3 to 14. \n",
    "\n",
    "accuracy_depth=[]\n",
    "\n",
    "for depth in range(3,15):\n",
    "\n",
    "    varied_tree = DecisionTreeClassifier(max_depth = depth, \n",
    "                                         random_state = 222)\n",
    "    varied_tree=varied_tree.fit(X_train,y_train)\n",
    "    tree_test_pred = varied_tree.predict(X_test)\n",
    "    tree_train_pred = varied_tree.predict(X_train)\n",
    "    accuracy_depth.append({'depth':depth,\n",
    "                           'test_accuracy':accuracy_score\\\n",
    "                           (y_test,tree_test_pred),\n",
    "                           'train_accuracy':accuracy_score\\\n",
    "                           (y_train,tree_train_pred)\n",
    "                          })\n",
    "    \n",
    "    print('Depth = %2.0f \\t Test Accuracy = %2.2f \\t \\\n",
    "    Training Accuracy = %2.2f'% (depth,accuracy_score\\\n",
    "                                (y_test, tree_test_pred),\n",
    "                                 accuracy_score(y_train, \n",
    "                                 tree_train_pred)))\n",
    "\n",
    "abd_df = pd.DataFrame(accuracy_depth)\n",
    "abd_df.index = abd_df['depth']\n",
    "\n",
    "# plot tree depth by accuracy\n",
    "fig, ax=plt.subplots(figsize=(6,2.5))\n",
    "\n",
    "ax.plot(abd_df.depth,abd_df.train_accuracy,\n",
    "        'ro-',label='Training Accuracy')\n",
    "ax.plot(abd_df.depth,abd_df.test_accuracy,\n",
    "        'bv--',label='Test Accuracy')\n",
    "\n",
    "plt.title('Varied Tree Depth by Accuracy')\n",
    "ax.set_xlabel('Tree Depth')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal maximum depth exists where test and training accuracy are both highest (60% and 64%, respectively). This is where depth is equal to 12. Now we can print the classification reports for both the un-tuned and tuned models, noting some improvement in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untuned Decision Tree Classifier\n",
    "untuned_tree = DecisionTreeClassifier(random_state=222)\n",
    "untuned_tree = untuned_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "untuned_tree1 = untuned_tree.predict(X_test)\n",
    "\n",
    "# accuracy and classification report\n",
    "print('Untuned Decision Tree Classifier')\n",
    "print('Accuracy Score')\n",
    "print(accuracy_score(y_test, untuned_tree1))\n",
    "print('Classification Report \\n', \n",
    "       classification_report(y_test, untuned_tree1))\n",
    "\n",
    "# Tuned Decision Tree Classifier\n",
    "tuned_tree = DecisionTreeClassifier(max_depth = 12,\n",
    "                                    random_state=222)\n",
    "tuned_tree = tuned_tree.fit(X_train, y_train)\n",
    "# Predict on test set\n",
    "tuned_tree1 = tuned_tree.predict(X_test)\n",
    "\n",
    "# accuracy and classification report\n",
    "print('Tuned Decision Tree Classifier')\n",
    "print('Accuracy Score')\n",
    "print(accuracy_score(y_test, tuned_tree1))\n",
    "print('Classification Report \\n', \n",
    "       classification_report(y_test, tuned_tree1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = ['Injured', 'Not injured']\n",
    "reduced_tree = DecisionTreeClassifier(max_depth = 2,\n",
    "                                      random_state=222)\n",
    "reduced_tree = reduced_tree.fit(X_train, y_train)\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "# plot pruned tree at a max depth of 2\n",
    "dot_data = export_graphviz(reduced_tree,\n",
    "feature_names = X_train.columns,\n",
    "class_names = cn,\n",
    "filled = True, out_file=None)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Tuning\n",
    "rf_train_accuracy = []\n",
    "rf_test_accuracy = []\n",
    "for n in range(1, 15):\n",
    "    rf = RandomForestClassifier(max_depth = n, \n",
    "                                random_state=222)\n",
    "    rf = rf.fit(X_train, y_train)\n",
    "    rf_pred_train = rf.predict(X_train)\n",
    "    rf_pred_test = rf.predict(X_test)\n",
    "    rf_train_accuracy.append(accuracy_score(y_train, \n",
    "                                            rf_pred_train))\n",
    "    rf_test_accuracy.append(accuracy_score(y_test, \n",
    "                                            rf_pred_test))\n",
    "    print('Max Depth = %2.0f \\t Test Accuracy = %2.2f \\t \\\n",
    "    Training Accuracy = %2.2f'% (n, accuracy_score(y_test,\n",
    "                                                  rf_pred_test),\n",
    "                               accuracy_score(y_train,\n",
    "                                              rf_pred_train)))\n",
    "max_depth = list(range(1, 15))\n",
    "fig, plt.subplots(figsize=(6,2.5))\n",
    "plt.plot(max_depth, rf_train_accuracy, 'bv--', \n",
    "         label='Training Accuracy')\n",
    "plt.plot(max_depth, rf_test_accuracy, 'ro--', \n",
    "         label='Test Accuracy')\n",
    "plt.title('Random Forest Accuracy')\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(max_depth)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untuned Random Forest\n",
    "untuned_rf = RandomForestClassifier(random_state=222)\n",
    "untuned_rf = untuned_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "untuned_rf1 = untuned_rf.predict(X_test)\n",
    "\n",
    "# accuracy and classification report\n",
    "print('Untuned Random Forest Model')\n",
    "print('Accuracy Score')\n",
    "print(accuracy_score(y_test, untuned_rf1))\n",
    "print('Classification Report \\n', \n",
    "       classification_report(y_test, untuned_rf1))\n",
    "\n",
    "# Tuned Random Forest\n",
    "tuned_rf = RandomForestClassifier(random_state=222,\n",
    "                                  max_depth = 12)\n",
    "tuned_rf = tuned_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "tuned_rf1 = tuned_rf.predict(X_test)\n",
    "\n",
    "# accuracy and classification report\n",
    "print('Tuned Random Forest Model')\n",
    "print('Accuracy Score')\n",
    "print(accuracy_score(y_test, tuned_rf1))\n",
    "print('Classification Report \\n', \n",
    "       classification_report(y_test, tuned_rf1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax3 = fig.add_subplot(223)\n",
    "\n",
    "# logistic regression confusion matrix\n",
    "plot_confusion_matrix(tuned_lr, X_test, y_test, ax=ax1)\n",
    "ax1.set_title('Logistic Regression')\n",
    "\n",
    "# Decision tree confusion matrix\n",
    "plot_confusion_matrix(tuned_tree, X_test, y_test, ax=ax2)\n",
    "ax2.set_title('Decision Tree')\n",
    "\n",
    "\n",
    "# random forest confusion matrix\n",
    "plot_confusion_matrix(tuned_rf, X_test, y_test, ax=ax3)\n",
    "ax3.set_title('Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract predicted probabilities from models\n",
    "tuned_lr_pred = tuned_lr.predict_proba(X_test)[:, 1]\n",
    "tuned_tree_pred = tuned_tree.predict_proba(X_test)[:, 1]\n",
    "tuned_rf_pred = tuned_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# plot all of the roc curves on one graph\n",
    "tuned_lr_roc = metrics.roc_curve(y_test,tuned_lr_pred)\n",
    "fpr,tpr,thresholds = metrics.roc_curve(y_test,tuned_lr_pred)\n",
    "tuned_lr_auc = metrics.auc(fpr, tpr)\n",
    "tuned_lr_plot = metrics.RocCurveDisplay(fpr=fpr,tpr=tpr, \n",
    "roc_auc = tuned_lr_auc, \n",
    "estimator_name = 'Logistic Regression')\n",
    "\n",
    "tuned_tree_roc = metrics.roc_curve(y_test,tuned_tree_pred)\n",
    "fpr,tpr,thresholds = metrics.roc_curve(y_test,tuned_tree_pred)\n",
    "tuned_tree_auc = metrics.auc(fpr, tpr)\n",
    "tuned_tree_plot = metrics.RocCurveDisplay(fpr=fpr,tpr=tpr,\n",
    "roc_auc=tuned_tree_auc, \n",
    "estimator_name = 'Decision Tree')\n",
    "\n",
    "tuned_rf1_roc = metrics.roc_curve(y_test, tuned_rf_pred)\n",
    "fpr,tpr,thresholds = metrics.roc_curve(y_test,tuned_rf_pred)\n",
    "tuned_rf1_auc = metrics.auc(fpr, tpr)\n",
    "tuned_rf1_plot = metrics.RocCurveDisplay(fpr=fpr,tpr=tpr, \n",
    "roc_auc=tuned_rf1_auc, \n",
    "estimator_name = 'Random Forest')\n",
    "\n",
    "# plot set up\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plt.title('ROC Curves for 3 Models',fontsize=12)\n",
    "plt.plot([0, 1], [0, 1], linestyle = '--', \n",
    "         color = '#174ab0')\n",
    "plt.xlabel('',fontsize=12) \n",
    "plt.ylabel('',fontsize=12) \n",
    "\n",
    "# Model ROC Plots Defined above\n",
    "tuned_lr_plot.plot(ax)\n",
    "tuned_tree_plot.plot(ax)\n",
    "tuned_rf1_plot.plot(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Performance Metrics\n",
    "report1 = classification_report(y_test,tuned_lr1,\n",
    "output_dict=True)\n",
    "accuracy1 = round(report1['accuracy'],4)\n",
    "precision1 = round(report1['1']['precision'],4)\n",
    "recall1 = round(report1['1']['recall'],4)\n",
    "fl_score1 = round(report1['1']['f1-score'],4)\n",
    "\n",
    "# Decision Tree Performance Metrics                       \n",
    "report2 = classification_report(y_test,tuned_tree1,\n",
    "output_dict=True)\n",
    "accuracy2 = round(report2['accuracy'],4)\n",
    "precision2 = round(report2['1']['precision'],4)\n",
    "recall2 = round(report2['1']['recall'],4)\n",
    "fl_score2 = round(report2['1']['f1-score'],4)\n",
    "\n",
    "# Random Forest Performance Metrics\n",
    "report3 = classification_report(y_test,tuned_rf1,\n",
    "output_dict=True)                \n",
    "accuracy3 = round(report3['accuracy'],4)\n",
    "precision3 = round(report3['1']['precision'],4)\n",
    "recall3 = round(report3['1']['recall'],4)\n",
    "fl_score3 = round(report3['1']['f1-score'],4)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = PrettyTable()\n",
    "table1.field_names = ['Model', 'Test Accuracy', \n",
    "                      'Precision', 'Recall', \n",
    "                      'F1-score']\n",
    "table1.add_row(['Logistic Regression', accuracy1, \n",
    "                precision1, recall1, fl_score1])\n",
    "table1.add_row(['Decision Tree', accuracy2, \n",
    "                precision2, recall2, fl_score2])\n",
    "table1.add_row(['Random Forest', accuracy3, \n",
    "                precision3, recall3, fl_score3])\n",
    "print(table1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean-Squared Errors\n",
    "mse1 = round(mean_squared_error(y_test, tuned_lr1),4)\n",
    "mse2 = round(mean_squared_error(y_test, tuned_tree1),4)\n",
    "mse3 = round(mean_squared_error(y_test, tuned_rf1),4)\n",
    "\n",
    "table2 = PrettyTable()\n",
    "table2.field_names = ['Model', 'AUC', 'MSE']\n",
    "table2.add_row(['Logistic Regression', \n",
    "                 round(tuned_lr_auc,4), mse1])\n",
    "table2.add_row(['Decision Tree', \n",
    "                 round(tuned_tree_auc,4), mse2])\n",
    "table2.add_row(['Random Forest', \n",
    "                 round(tuned_rf1_auc,4), mse3])\n",
    "print(table2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**  \n",
    "\n",
    "Shmueli, G., Bruce, P. C., Gedeck, P., & Patel, N. R. (2020). *Data mining for business  \n",
    "analytics: Concepts, techniques and applications in Python.* John Wiley & Sons, Inc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
