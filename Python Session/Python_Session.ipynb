{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Introductory Workshop by University of San Diego (USD)  \n",
    "**Leonid Shpaner**  \n",
    "**January 21, 2022**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas JupyterLab and Jupyter Notebook are the two most commonly used interactive computing platforms warehoused within the Anaconda distribution, data scientists can also leverage the cloud-based coding environment of Google Colab.\n",
    "\n",
    "[https://colab.research.google.com/?utm_source=scs-index](https://colab.research.google.com/?utm_source=scs-index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JupyterLab Basics\n",
    "[https://jupyterlab.readthedocs.io/en/stable/user/interface.html](https://jupyterlab.readthedocs.io/en/stable/user/interface.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a code cell/block!\n",
    "\n",
    "# basic for loop\n",
    "for x in [1,2,3]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a markdown cell!  \n",
    "### Level 3 Heading  \n",
    "*italics characters are surrounded by one asterisk*  \n",
    "**bold characters are surrounded by two asterisks**  \n",
    "* one asterisk in front of an item of text can serve as a bullet point.  \n",
    "* to move the text to the next line, ensure to enter two spaces after the line.   \n",
    "    1. Simply number the items on a list using normal numering schema.  \n",
    "    2. If you have numbers in the same markdown cell as bullet points (i.e., below),\n",
    "    3. skip 4 spaces after the last bullet point and then begin numbering.\n",
    "\n",
    "Here's a guide for syntax: https://www.markdownguide.org/basic-syntax/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a code cell to conduct basic math operations as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order of operations in Python is just as important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+3*10 == (1+3)*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more basic operations including but not limited to taking the square root, log, and generally more advanced mathematical operations, we will have to import our first library (math) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we run two consecutive lines below without assigning to a variable or parsing in a print statement ahead of the function calls, only the latest (last) function will print the output of what is in the cell block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.sqrt(9)\n",
    "math.log10(100)\n",
    "math.factorial(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try this again, by adding `print()` statements in front of the three functions, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the output on multiple lines\n",
    "print(math.sqrt(9))\n",
    "print(math.log10(100))\n",
    "print(math.factorial(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a string?\n",
    "\n",
    "A string is simply any open alpha/alphanumeric text characters surrounded by either single quotation marks or double quotation marks, with no preference assigned for either single or double quotation marks, with a print statement being called prior to the strings. For example,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This is a string.')\n",
    "print( \"This is also a string123.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strings in Python are arrays of bytes that represent \"Unicode characters\" (GeeksforGeeks, 2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike R, Python uses the '=' symbol for making assignment statements. we can `print()` what is contained in the assignment or just call the assignment to the string as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheese = 'pepper jack'\n",
    "cheese "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining and setting the current working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of determining and setting the working directory cannot be stressed enough. \n",
    "  1. Run `import os` to import operating system module. \n",
    "  2. then assign `os.getcwd()` to `cwd`.\n",
    "  3. You may choose to print the working directory using the print function as shown below.\n",
    "  4. Or change the working directory by running `os.chdir('')`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # import the operating system module\n",
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Change the current working directory\n",
    "# os.chdir('')\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install most common libraries, simply type in the command `pip install` `followed by library name` into an empty code cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data science applications, we most commonly use [pandas](https://pandas.pydata.org/pandas-docs/stable/) for \"data structures and data analysis tools\" (Pandas, 2021) and [NumPy](https://numpy.org/) for \"scientific computing with Python\" (Numpy.org, n.d.). \n",
    "\n",
    "Ideally, at the beginning of a project, we will create an empty cell block that loads all of the libraries that we will be using. However, as we progress throughout this tutorial, we will load the necessary libraries separately.\n",
    "\n",
    "Let us now load these two libraries into an empty code cell block using `import` `name of the library` `as` `abbreviated form`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, `Python` throws warning messages in pink on the output of code cell blocks that otherwise run correctly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.warn('This is an example warning message.') # displaying warning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, it is helpful to see what these warnings are saying as added layers of de-bugging. However, they may also create unsightly output. For this reason, we will suppress any and all warning messages for the remainder of this tutorial.  \n",
    "\n",
    "To disable/suppress warning messages, let us write the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Type (string): `str`  \n",
    "Numeric Types:      `int`, `float`, `complex`  \n",
    "Sequence Types:\t    `list`, `tuple`, `range`  \n",
    "Mapping Type:\t    `dict` - dictionary (used to store key:value pairs)  \n",
    "Logical:\t        `bool` - boolean  (True or False)   \n",
    "Binary Types:\t    `bytes`, `bytearray`, `memoryview`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert an integer to a string. We do this using the `str()` function. Recall, how in R, this same function call is designated for something completely different - inspecting the structure of the dataframe.\n",
    "\n",
    "We can also examine `floats` and `bools` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the variable to an int\n",
    "int_numb  = 2356\n",
    "print('Integer:', int_numb)\n",
    "\n",
    "# assign the variable to a float\n",
    "float_number = 2356.0\n",
    "print('Float:', float_number)\n",
    "\n",
    "# convert the variable to a string\n",
    "str_numb = str(int_numb)\n",
    "print('String:',str_numb)\n",
    "\n",
    "# convert variable from float to int\n",
    "int_number = int(float_number)\n",
    "\n",
    "# boolean\n",
    "bool1 = 2356 > 235\n",
    "bool2 = 2356 == 235\n",
    "print(bool1)\n",
    "print(bool2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures \n",
    "\n",
    "**What is a variable?**\n",
    "A variable is a container for storing a data value, exhibited as a reference to \"to an object in memory which means that whenever a variable is assigned to an instance, it gets mapped to that instance. A variable in R can store a vector, a group of vectors or a combination of many R objects\" (GeeksforGeeks, 2020). \n",
    "\n",
    "There are 3 most important data structures in Python: vector, matrix, and dataframe.  \n",
    "\n",
    "**Vector**:  the most basic type of data structure within R; contains a series of values of the same data class. It is a \"sequence of data elements\" (Thakur, 2018). \n",
    "\n",
    "**Matrix**: a 2-dimensional version of a vector. Instead of only having a single row/list of data, we have rows and columns of data of the same data class.\n",
    "\n",
    "**Dataframe**: the most important data structure for data science. Think of dataframe as loads of vectors pasted together as columns. Columns in a dataframe can be of different data class, but values within the same column must be the same data class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a one-dimensional horizontal list as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [0, 1, 2, 3]\n",
    "list1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or a one-dimensional vertical list as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2 = [[1],\n",
    "         [2],\n",
    "         [3],\n",
    "         [4]]\n",
    "list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors and Their Operations\n",
    "Now, to vectorize these lists, we simply assign it to the `np.array()` function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = np.array(list1)\n",
    "print(vector1)\n",
    "print('\\n') # for printing an empty new line \n",
    "            # between outputs\n",
    "vector2 = np.array(list2)\n",
    "print(vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the following basic between vector arithmetic operations (addition, subtraction, and division, respectively) changes the resulting data structures from one-dimensional arrays to two-dimensional matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding vector 1 and vector 2\n",
    "addition = vector1 + vector2\n",
    "\n",
    "# subtracting vector 1 and vector 2\n",
    "subtraction = vector1 - vector2\n",
    "\n",
    "# multiplying vector 1 and vector 2\n",
    "multiplication = vector1 * vector2\n",
    "\n",
    "# divifing vector 1 by vector 2\n",
    "division = vector1 / vector2\n",
    "\n",
    "# Now let's print the results of these operations\n",
    "print('Vector Addition: ', '\\n', addition, '\\n')\n",
    "print('Vector Subtraction:', '\\n', subtraction, '\\n')\n",
    "print('Vector Multiplication:', '\\n', multiplication, '\\n')\n",
    "print('Vector Division:', '\\n', division)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, a vector of logical strings will contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector3 = np.array([True, False, True, False, True])\n",
    "vector3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas in R, we use the `length()` function to measure the length of an object (i.e., vector, variable, or dataframe), we apply the `len()` function in `Python` to determine the number of members inside this object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vector3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us say for example, that we want to access the third element of `vector1` from what we defined above. In this case, the syntax is the same as in R. We can do so as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now say we want to access the first, fifth, and ninth elements of this dataframe. To this end, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector4 = np.array([1,3,5,7,9,20,2,8,10,35,76,89,207])\n",
    "vector4_index = vector4[1], vector4[5], vector4[9]\n",
    "vector4_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wish to access the third element on the first row of this matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create (define) new matrix\n",
    "matrix1 = np.array([[1,2,3,4,5], [6,7,8,9,10], \n",
    "                [11,12,13,14,15]])\n",
    "print(matrix1)\n",
    "\n",
    "print('\\n','3rd element on 1st row:', matrix1[0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Numbers and Accessing Elements in Python\n",
    "\n",
    "Whereas it would make sense to start counting items in an array with the number `1` like we do in R, this is not true in Python. We `ALWAYS` start counting items with the number `0` as the first number of any given array in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to access certain elements within the dataframe? For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the length of vector 1\n",
    "print(len(vector1))\n",
    "\n",
    "# get all elements\n",
    "print(vector1[0:4])\n",
    "\n",
    "# get all elements except last one\n",
    "print(vector1[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock Dataframe Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike R, creating a dataframe in Python involves a little bit more work. For example, we will be using the pandas library to create what is called a pandas dataframe using the `pd.DataFrame()` function and map our variables to a dictionary. Like we previously discussed, a dictionary is used to index key:value pairs and to store these mapped values. Dictionaries are always started (created) using the `{` symbol, followed by the `name` in `quotation marks`, a `:`, and an opening `[`. They are ended using the opposite closing symbols.\n",
    "\n",
    "Let us create a mock dataframe for five fictitious individuals representing different ages, and departments at a research facility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Name': ['Jack', 'Kathy', 'Latesha', \n",
    "                    'Brandon', 'Alexa', \n",
    "                    'Jonathan', 'Joshua', 'Emily', \n",
    "                    'Matthew', 'Anthony', 'Margaret', \n",
    "                    'Natalie'],\n",
    "                   \n",
    "                   'Age':[47, 41, 23, 55, 36, 54, 48, \n",
    "                          23, 22, 27, 37, 43],\n",
    "                   \n",
    "                   'Experience':[7,5,9,3,11,6,8,9,5,2,1,4],\n",
    "                   'Position': ['Economist', \n",
    "                    'Director of Operations', \n",
    "                    'Human Resources', 'Admin. Assistant', \n",
    "                    'Data Scientist', 'Admin. Assistant', \n",
    "                    'Account Manager', 'Account Manager', \n",
    "                    'Attorney', 'Paralegal','Data Analyst', \n",
    "                    'Research Assistant']})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Structure of a Dataframe\n",
    "\n",
    "Let us examine the structure of the dataframe. Once again, recall that whereas in R we would use `str()` to look at the structure of a dataframe, in Python, `str()` refers to string. Thus, we will use the `df.dtypes`, `df.info()`, `len(df)`, and `df.shape` operations/functions, respectively to examine the dataframe's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes, '\\n') # data types\n",
    "print(df.info(), '\\n') # more info on dataframe\n",
    "\n",
    "# print length of df (rows)\n",
    "print('Length of Dataframe:', len(df),\n",
    "      '\\n')\n",
    "\n",
    "# number of rows of dataframe\n",
    "print('Number of Rows:', df.shape[0])\n",
    "\n",
    "# number of columns of dataframe\n",
    "print('Number of Columns:', df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Data\n",
    "\n",
    "Let us say that now we want to sort this dataframe in order of age (youngest to oldest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas sorts values in ascending order\n",
    "# by default, so there is no need to parse \n",
    "# in ascending=True as a parameter\n",
    "\n",
    "df_age = df.sort_values(by=['Age'])\n",
    "df_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to also sort by experience while keeping age sorted according to previous specifications, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age_exp = df.sort_values(by = ['Age', 'Experience'])\n",
    "df_age_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling *#NA* values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*#NA* (not available) refers to missing values. What if our dataset has missing values? How should we handle this scenario? For example, age has some missing values.\n",
    "\n",
    "However, in our particular case, we have introduced `NaNs`. NaN simply refers to Not a Number. Since we are looking at age as numeric values, let us observe when two of those numbers appear as missing values of the NaN form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.DataFrame({'Name': ['Jack', 'Kathy', 'Latesha', \n",
    "                    'Brandon', 'Alexa', \n",
    "                    'Jonathan', 'Joshua', 'Emily', \n",
    "                    'Matthew', 'Anthony', 'Margaret', \n",
    "                    'Natalie'],\n",
    "                   \n",
    "                   'Age':[47, np.nan , 23, 55, 36, 54, 48, \n",
    "                          np.nan, 22, 27, 37, 43],\n",
    "                   \n",
    "                   'Experience':[7,5,9,3,11,6,8,9,5,2,1,4],\n",
    "                   'Position': ['Economist', \n",
    "                    'Director of Operations', \n",
    "                    'Human Resources', 'Admin. Assistant', \n",
    "                    'Data Scientist', 'Admin. Assistant', \n",
    "                    'Account Manager', 'Account Manager', \n",
    "                    'Attorney', 'Paralegal','Data Analyst', \n",
    "                    'Research Assistant']})\n",
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting *#NA* values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect dataset for missing values\n",
    "# with logical (bool) returns\n",
    "print(df_2.isnull(), '\\n')\n",
    "\n",
    "# sum up all of the missing values in \n",
    "# each row (if there are any)\n",
    "print(df_2.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can delete the rows with missing values by making an `dropna()` function call in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values\n",
    "df_2.dropna(subset=['Age'], inplace=True)  \n",
    "\n",
    "# inspect the dataframe; there are no\n",
    "# more missing values, since we dropped them\n",
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we receive a dataframe that, at a cursory glance, warehouses numerical values where we see numbers, but when running additional operations on the dataframe, we discover that we cannot conduct numerical exercises with columns that appear to have numbers. This is exactly why it is of utmost importance for us to always inspect the structure of the dataframe using the `df.dtypes` function call. Here is an example of the same dataframe with altered data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.DataFrame({\n",
    "                    'Name': ['Jack', 'Kathy', 'Latesha', \n",
    "                    'Brandon', 'Alexa', \n",
    "                    'Jonathan', 'Joshua', 'Emily', \n",
    "                    'Matthew', 'Anthony', 'Margaret', \n",
    "                    'Natalie'],\n",
    "                   \n",
    "                    'Age':['47', '41', '23', '55', '36', '54', \n",
    "                          '48', '23', '22', '27', '37', '43'],\n",
    "                   \n",
    "                    'Experience':[7,5,9,3,11,6,8,9,5,2,1,4],\n",
    "                    'Position': ['Economist', \n",
    "                    'Director of Operations', \n",
    "                    'Human Resources', 'Admin. Assistant', \n",
    "                    'Data Scientist', 'Admin. Assistant', \n",
    "                    'Account Manager', 'Account Manager', \n",
    "                    'Attorney', 'Paralegal','Data Analyst', \n",
    "                    'Research Assistant']\n",
    "                })\n",
    "df_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a cursory glance, the data frame looks identical to the `df` we had originally. However, inspecting the data types yields unexpected information, that age is not an integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age is now an object\n",
    "df_3.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert age back to an integer and re-inspect the dataframe. Notice how converting entire columns of dataframes from an objects to numeric data is more than just calling the `int()` function. We re-assign the variable with the specified column back to itself using the `pd.to_numeric()` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3['Age'] = pd.to_numeric(df_3['Age'])\n",
    "df_3.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, to cast a variable in a dataframe into an object (i.e., string), we can simply apply the `str()` function call before the dataframe name and specified column in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3['Experience'] = str(df_3['Age'])\n",
    "df_3.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from flat .csv file\n",
    "\n",
    "Assuming that your file is located in the same working directory that you have specified at the onset of \n",
    "this tutorial/workshop, make an assignment to a new variable (i.e., `ex_csv`) \n",
    "and call `pd.read_csv()` in the following generalized format:\n",
    "\n",
    "Notice that the `pd` in front of `read_csv()` belongs to the pandas library which we imported as `pd` earlier. \n",
    "\n",
    "`ex_csv <- pd.read.csv(filename)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying a Random State/Seed\n",
    "\n",
    "Whereas in R, we use the `set.seed()` command to specify an arbitrary number for reproducibility of results, in `Python` we use the `random_state()`function. It is always best practice to use the same assigned random state throughout the entire experiment. Setting the random state to this arbitrary number (of any length) will guarantee exactly the same output across all Python notebooks, sessions and users, respectively. \n",
    "\n",
    "When working with simulated numpy arrays, it is best practice to set a seed using the `np.random.seed()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a new data frame of numbers 1 - 100 and go over the basic statistical functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystats = pd.DataFrame(list(range(1,101)))\n",
    "mystats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = mystats.mean() # mean of the vector\n",
    "median = mystats.median() # median of the vector\n",
    "minimum = mystats.min() # minimum of the vector\n",
    "maximum = mystats.max() # maximum of the vector\n",
    "range_mystats = (mystats.min(),mystats.max())\n",
    "sum_mystats = mystats.sum() # sum of the vector\n",
    "stdev = mystats.std() # standard deviation of the vector\n",
    "summary = mystats.describe() # summary of the dataset\n",
    "\n",
    "# we put a '0' in brackets after each of the following\n",
    "# variables so that we can access only the respective\n",
    "# statistics of each function which are contained in \n",
    "# the first element\n",
    "print('Mean:', mean[0])\n",
    "print('Median:', median[0])\n",
    "print('Minimum:', minimum[0])\n",
    "print('Maximum:', maximum[0])\n",
    "print('Sum of values:', sum_mystats[0])\n",
    "print('Standard Deviation:', stdev[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can simply this endeavor by using the `df.describe()` function which will output the summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystats.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposing The Contents of a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish to transpose this dataframe, we can place a `.T` behind `.describe()` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystats.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating a Random Normal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "np.random.seed(222)\n",
    "\n",
    "mu, sigma = 50, 10 # mean and standard deviation\n",
    "\n",
    "# assign variable to a dataframe\n",
    "norm_vals = pd.DataFrame(np.random.normal(mu, sigma, 100))\n",
    "\n",
    "x = list(range(1, 101))\n",
    "y = list(norm_vals[0])\n",
    "\n",
    "norm_vals = pd.DataFrame(y,x)\n",
    "norm_vals.reset_index(inplace=True)\n",
    "norm_vals.rename(columns = {0:'Number',\n",
    "                              'index': 'Index'}, \n",
    "                               inplace = True) # mod. w/out copy\n",
    "norm_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Basic Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike `R` where we can simply call the `stem()` function to create a stem-and-leaf plot, in `Python` we must first install and import the `stemgraphic` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install stemgraphic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can limit the output of how many rows get printed in the resulting output by parsing in the `.loc()` function. So, if we want to print only the first 25 rows, we will access our dataframe, `norm_vals['Number']` followed by `.loc[0:25]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import stemgraphic\n",
    "\n",
    "# create stem-and-leaf plot\n",
    "fig, ax = stemgraphic.stem_graphic(norm_vals['Number'].iloc[0:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matplotlib Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must first import the `matplotlib` library, a most frequently used graphical library for making basic plots in Python. Let us import this library and plot the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can proceed to plot a histogram of these `norm_vals` in order to inspect their distribution from a purely graphical standpoint.  \n",
    "\n",
    "Unlike R, Python does not use a built-in `hist()` function to accomplish this task. To make the plot, we will parse in our `dataframe` follows by `.hist(grid = False)` where `grid = False` explicitly avoids plotting on a grid. Moreover, `plt.show()` expressly tells `matplotlib` to avoid extraneous output above the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a basic histogram\n",
    "norm_vals['Number'].hist(grid = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our title, x-axis, and y-axis labels are given to us by default. However, let us say that we want to change all of them to our desired specifications. To this end, we can parse in and  control the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_vals['Number'].hist(grid=False, \n",
    "               color = \"lightblue\") # change color\n",
    "plt.title ('Histogram of Simulated Data') # title\n",
    "plt.xlabel('Values')                      # x-axis\n",
    "plt.ylabel('Count')                       # y-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can make a boxplot using the `df.boxplot()` function call. However, the `norm_vals` dataframe has two columns. Let us only examine the randomly distributed 100 rows that we have contained in the `Number` column. \n",
    "\n",
    "One way to do this is to create a new dataframe to only access the `Number` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_number = pd.DataFrame(norm_vals['Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_number.boxplot(grid = False) \n",
    "\n",
    "# plot title\n",
    "plt.title ('Boxplot of Simulated Data') \n",
    "# x-axis label\n",
    "plt.xlabel('')   \n",
    "# y-axis label\n",
    "plt.ylabel('Values')  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us pivot the boxplot by parsing in the `vert=False` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_number.boxplot(grid = False, vert = False) # re-orient\n",
    "plt.title ('Boxplot of Simulated Data') # title\n",
    "plt.ylabel('Values')                    # y-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a simple scatter plot, we will call the `plot.scatter()` function on the dataframe as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = norm_vals['Index'] # independent variable\n",
    "y = norm_vals['Number'] # dependent variable\n",
    "\n",
    "fig,ax = plt.subplots(figsize = (10,4))\n",
    "plt.scatter(x, y) # scatter plot call\n",
    "plt.title('Scatter Plot of Simulated Data')\n",
    "\n",
    "# we can also separate lines by semicolon\n",
    "plt.xlabel('Index'); plt.ylabel('Value'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile-Quantile Plot\n",
    "\n",
    "Let us create a vector from simulated data for the next example and generate a normal quantile plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(222)\n",
    "quant_ex = np.random.normal(0, 1, 100) \n",
    "\n",
    "sm.qqplot(quant_ex)\n",
    "plt.title('Normal Q-Q Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now add a theoretical Q-Q line at a 45 degree angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(222)\n",
    "quant_ex = np.random.normal(0, 1, 100) \n",
    "\n",
    "sm.qqplot(quant_ex, \n",
    "          line='45') # theoretical Q-Q line\n",
    "plt.title('Normal Q-Q Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewness and Box-Cox Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From statistics, let us recall that if the mean is greater than the median, the distribution will be  positively skewed. Conversely, if the median is greater than the mean, or the mean is less than the \n",
    "median, the distribution will be negatively skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_norm_vals = norm_vals['Number'].mean()\n",
    "median_norm_vals = norm_vals['Number'].median()\n",
    "\n",
    "print('Mean of norm_vals:', mean_norm_vals) \n",
    "print('Median of norm_vals:', median_norm_vals)\n",
    "\n",
    "print('Difference =', mean_norm_vals - median_norm_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both the mean and the median values are fairly close together, the data appears to be normally distributed, so we will simulate another example involving skewness.\n",
    "\n",
    "Whereas in R, we use the all-encompassing `caret` machine learning library to handle multiple tasks, often we find ourselves loading more libraries in `Python` like the `scipy` library to handle Box-Cox transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "original_data = np.random.exponential(size = 1000)\n",
    "\n",
    "# transform training data & save lambda value\n",
    "fitted_data, fitted_lambda = stats.boxcox(original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data1 = pd.DataFrame(original_data)\n",
    "fitted_data1 = pd.DataFrame(fitted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original hist(0)\n",
    "original_data1.hist(figsize=(10,4)) \n",
    "plt.title ('Histogram of Original Data')    \n",
    "plt.xlabel('Values'); plt.ylabel('Count')                         \n",
    "\n",
    "# transformed hist() \n",
    "fitted_data1.hist(grid = False, \n",
    "                  figsize=(10,4))              \n",
    "plt.title ('Histogram of Transformed Data') \n",
    "plt.xlabel('Values'); plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set up an example dataset for the  following modeling endeavors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be accessing Python's most commonly used machine learning library, [`scikit-learn`](https://scikit-learn.org/stable/) to build the ensuing algorithms, though there are others like [`pycaret`](https://pycaret.org/) and [`SciPy`](https://scipy.org/), to name a few.\n",
    "\n",
    "So, let us go ahead and import `sklearn` for linear regression into our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice a more refined importing syntax, atypical of the standard `import library name`. We are telling `Python` to import the Linear Regression module from the `scikit-learn` library as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_mod = pd.DataFrame({\n",
    "                # X1\n",
    "                'Hydrogen':[.18,.20,.21,.21,.21,.22,.23,\n",
    "                            .23,.24,.24,.25,.28,.30,.37,.31,\n",
    "                            .90,.81,.41,.74,.42,.37,.49,.07,\n",
    "                            .94,.47,.35,.83,.61,.30,.61,.54],  \n",
    "                # X2\n",
    "                'Oxygen':[.55,.77,.40,.45,.62,.78,.24,.47,\n",
    "                          .15,.70,.99,.62,.55,.88,.49,.36,\n",
    "                          .55,.42,.39,.74,.50,.17,.18,.94,\n",
    "                          .97,.29,.85,.17,.33,.29,.85], \n",
    "                # X3\n",
    "                'Nitrogen':[.35,.48,.31,.75,.32,.56,.06,.46,\n",
    "                            .79,.88,.66,.04,.44,.61,.15,.48,\n",
    "                            .23,.90,.26,.41,.76,.30,.56,.73,\n",
    "                            .10,.01,.05,.34,.27,.42,.83], \n",
    "                # y\n",
    "                'Gas Porosity':[.46,.70,.41,.45,.55,\n",
    "                                .44,.24,.47,.22,.80,.88,.70,\n",
    "                                .72,.75,.16,.15,.08,.47,.59,\n",
    "                                .21,.37,.96,.06,.17,.10,.92,\n",
    "                                .80,.06,.52,.01,.37]})\n",
    "lin_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = lin_mod['Hydrogen']; x2 = lin_mod['Oxygen']\n",
    "x3 = lin_mod['Nitrogen']; y = lin_mod['Gas Porosity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to modeling, it is best practice to examine correlation visa vie visual scatterplot analysis as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (10,4)) # resize plot\n",
    "plt.scatter(x1, y)\n",
    "plt.title('Gas Porosity vs. Hydrogen') \n",
    "plt.xlabel('Hydrogen Content'); plt.ylabel('Gas Porosity')          \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us calculate our correlation coefficient for the first variable relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr1 = np.corrcoef(x1, y)\n",
    "r1 = corr1[0,1]\n",
    "r1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the correlation coefficient r you will see that there exists a relatively moderate (positive) relationship. Let us now build a simple linear model from this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the additional brackets\n",
    "# we do this to specify columns\n",
    "# within our dataframe of interest\n",
    "X1 = lin_mod[['Hydrogen']]\n",
    "y = lin_mod[['Gas Porosity']]\n",
    "\n",
    "# set-up the linear regression\n",
    "lm_model1 = LinearRegression().fit(X1, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will rely on the stats model package to obtain a summary output table. Here, it is important to note that unlike in `R`, the `statsmodels` package in Python does not add a constant to the summary output, so for reproducible results, we will add it by the `sm.add_constant()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.api import OLS\n",
    "X1 = sm.add_constant(X1)\n",
    "X1_results = OLS(y,X1).fit()\n",
    "X1_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the p-value for hydrogen content is 0.196, which lacks statistical significance when compared to the alpha value of 0.05 (at the 95% confidence level). Moreover, the *R*-Squared value of 0.057 suggests that roughly 6% of the variance for gas propensity is explained by hydrogen content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the same scatter plot, but this time with a best fit line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "plt.scatter(x1, y) \n",
    "plt.title('Gas Porosity vs. Hydrogen')\n",
    "plt.xlabel('Hydrogen Content')       \n",
    "plt.ylabel('Gas Porosity')           \n",
    "\n",
    "# create best-fit line based on slope-intercept form\n",
    "m, b = np.polyfit(x1, y, 1)\n",
    "plt.plot(x1, m*x1 + b, \n",
    "         color = 'red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To account for all independent (x) variables in the model, let us set up the model in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lin_mod[['Hydrogen', 'Oxygen', 'Nitrogen']]\n",
    "y = lin_mod[['Gas Porosity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the remaining variable relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "plt.scatter(x2,y)\n",
    "plt.title('Gas Porosity vs. Oxygen')\n",
    "plt.xlabel('Oxygen Content')     \n",
    "plt.ylabel('Gas Porosity')           \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3_plot =plt.scatter(x3,y) # create scatter plot\n",
    "plt.title('Gas Porosity vs. Nitrogen') # title\n",
    "plt.xlabel('Nitrogen Content')       # x-axis label\n",
    "plt.ylabel('Gas Porosity')           # y-axis label\n",
    "x3_plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(X)\n",
    "lin_model_results = OLS(y,X).fit()\n",
    "lin_model_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas in linear regression, it is necessary to have a quantitative and continuous target variable, logistic regression is part of the generalized linear model series that has a categorical (often binary)\n",
    "target (outcome) variable. For example, let us say we want to predict grades for mathematics courses taught at a university. So, we have the following example dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_df = pd.DataFrame(\n",
    "    {'Calculus1':[56,80,10,8,20,90,38,42,57,58,90,2,\n",
    "                  34,84,19,74,13,67,84,31,82,67,99,\n",
    "                  76,96,59,37,24,3,57,62],\n",
    "     'Calculus2':[83,98,50,16,70,31,90,48,67,78,55,\n",
    "                  75,20,80,74,86,12,100,63,36,91,\n",
    "                  19,69,58,85,77,5,31,57,72,89],\n",
    "     'linear_alg':[87,90,85,57,30,78,75,69,83,85,90,\n",
    "                   85,99,97, 38,95,10,99,62,47,17,\n",
    "                   31,77,92,13,44,3,83,21,38,70],\n",
    "     'pass_fail':['P','F','P','F','P','P','P','P',\n",
    "                  'F','P','P','P','P','P','P','F',\n",
    "                  'P','P','P','F','F','F','P','P',\n",
    "                  'P','P','P','P','P','P','P']})\n",
    "math_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this juncture, we cannot build a model with categorical values until and unless they are binarized using a dictionary mapping.  A passing score will be designated by a 1, and failing score with a 0, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize pass fail to 1 = pass, 0=fail\n",
    "# into new column\n",
    "math_df['math_outcome'] = math_df['pass_fail'].map({'P':1,'F':0})\n",
    "math_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the Linear Regression module from the scikit-learn library as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of `OLS.fit()` like we did for linear regression, we will be using the `sm.Logit()` function call to pass in our *y* and *x*, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also drop the columns that we \n",
    "# will not be using\n",
    "logit_X = math_df.drop(columns=['pass_fail', \n",
    "                                'math_outcome'])\n",
    "logit_X = sm.add_constant(logit_X)\n",
    "logit_y = math_df['math_outcome']\n",
    "\n",
    "# notice the sm.Logit() function call\n",
    "log_results = sm.Logit(logit_y,logit_X).fit()\n",
    "log_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the Decision Tree Classifier from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the `mtcars` dataset from `R`, and will have to import from `statsmodels` into `Python` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcars = sm.datasets.get_rdataset(\"mtcars\", \"datasets\", cache=True).data\n",
    "mtcars = pd.DataFrame(mtcars)\n",
    "mtcars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mtcars.dtypes, '\\n')\n",
    "print('Number of Rows:',mtcars.shape[0])\n",
    "print('Number of Columns:',mtcars.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from float to int\n",
    "# otherwise DT won't run properly\n",
    "mtcars = mtcars.astype(int)\n",
    "print(mtcars.dtypes, '\\n')\n",
    "print('Number of Rows:',mtcars.shape[0])\n",
    "print('Number of Columns:',mtcars.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we did for the logistic regression example, let us now create our *x* and *y* variables from this `mtcars` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcars_X = mtcars.drop(columns=['mpg'])\n",
    "mtcars_y = mtcars[['mpg']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without importing any special graphical libraries, the decision tree output plot will look condensed, small, and virtually unreadable. We can comment out the figure size dimensions, but that still will not produce anything sophisticated in nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(figsize = (30,30))\n",
    "tree_model = DecisionTreeClassifier(max_depth=2)\n",
    "tree_model = tree_model.fit(mtcars_X, mtcars_y)\n",
    "tree_plot = tree.plot_tree(tree_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to label our `x` and `y` variables is to assign the `x` variables to a list and use the `.remove()` function to remove our target variable from that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_var = list(mtcars.columns)\n",
    "target = 'mpg'\n",
    "X_var.remove(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more sophisticated graphical output, we can tap into scikit-learn's `export_graphviz` package in conjunction with another library called `pydotplus` which we will need to install separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(tree_model,\n",
    "                           feature_names = X_var,\n",
    "                           filled = True, \n",
    "                           out_file = None)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Modeling and Cross-Validation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, \\\n",
    "y_test = train_test_split(mtcars_X, mtcars_y, \n",
    "                          test_size = 0.25, \n",
    "                          random_state = 222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shape of train and test sets\n",
    "train_shape = X_train.shape[0]\n",
    "test_shape = X_test.shape[0]\n",
    "\n",
    "# calculate the proportions of each, respectively\n",
    "train_percent = train_shape/(train_shape + test_shape)\n",
    "test_percent = test_shape/(train_shape + test_shape)\n",
    "\n",
    "print('Train Size:', train_percent) \n",
    "print('Test Size:', test_percent) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us bring in a generalized linear model for this illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcars_X = mtcars.drop(columns=['mpg'])\n",
    "mtcars_y = mtcars[['mpg']]\n",
    "\n",
    "# notice the sm.Logit() function call\n",
    "mtcars_model = sm.add_constant(X_train)\n",
    "\n",
    "# back to the linear model since target\n",
    "# variable is quantitative and continuous\n",
    "mtcars_model_results = OLS(y_train, X_train).fit()\n",
    "mtcars_model_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can cross-validate, let us run our predictions of miles per gallon (mpg) on our holdout (test-set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcars_mod = LinearRegression()\n",
    "mtcars_mod.fit(X_train, y_train)\n",
    "y_pred = mtcars_mod.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our predictions remain nested in an array, we will bring in a baseline measure from the `scikit-learn` library as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from numpy import mean\n",
    "from numpy import absolute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cross-validation method to use\n",
    "cv = KFold(n_splits=5, random_state=222, shuffle=True)\n",
    "\n",
    "# use k-fold CV to evaluate model\n",
    "scores = cross_val_score(mtcars_mod, X_train, y_train, \n",
    "                         scoring='neg_mean_absolute_error',\n",
    "                         cv=cv, n_jobs=-1)\n",
    "\n",
    "# view mean absolute error\n",
    "ma_scores = mean(absolute(scores))\n",
    "ma_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cluster is a collection of observations. We want to group these observations based on the most similar attributes. We use distance measures to measure similarity between clusters. This is one of the most widely-used unsupervised learning techniques that groups \"similar data points together and discover underlying patterns. To achieve this objective, K-means looks for a fixed number (*k*) of clusters in a dataset\" (Garbade, 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *k* in k-means is the fixed number of centroids (center of cluster) for which the algorithm will take the mean for based on the number of clusters (collection of data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary library\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us split the mtcars dataset into 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3).fit(mtcars)\n",
    "centroids = kmeans.cluster_centers_\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans1 = KMeans(n_clusters=3, \n",
    "                 random_state=222).fit(mtcars)\n",
    "centroids1 = pd.DataFrame(kmeans1.cluster_centers_,\n",
    "columns = mtcars.columns)\n",
    "pd.set_option('precision', 3)\n",
    "centroids1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withinClusterSS = [0] * 3\n",
    "clusterCount = [0] * 3\n",
    "for cluster, distance in zip(kmeans1.labels_,\n",
    "                             kmeans1.transform(mtcars)):\n",
    "    withinClusterSS[cluster] += distance[cluster]**2\n",
    "    clusterCount[cluster] += 1\n",
    "for cluster, withClustSS in enumerate(withinClusterSS):\n",
    "    print('Cluster {} ({} members): {:5.2f} within cluster'.format(cluster,\n",
    "clusterCount[cluster], withinClusterSS[cluster]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is the appropriate number of clusters that we should generate? Can we do better with more clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create segments using K-means clustering\n",
    "# using elbow method to find no of clusters\n",
    "wcss=[]\n",
    "for i in range(1,7):\n",
    "    kmeans= KMeans(n_clusters = i, \n",
    "                   init = 'k-means++', \n",
    "                   random_state = 222)\n",
    "    kmeans.fit(mtcars_X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "print(wcss)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "plt.plot(range(1,7), \n",
    "         wcss, linestyle='--', \n",
    "         marker='o', \n",
    "         label='WCSS value')\n",
    "plt.title('WCSS Value: Elbow method')\n",
    "plt.xlabel('# of clusters: K value')\n",
    "plt.ylabel('WCSS Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another form of unsupervised learning type of cluster analysis, which takes on a more visual method, working particularly well with smaller samples (i.e., n < 500), such as this `mtcars` dataset. We start out with as many clusters as observations, and we go through a procedure of combining observations into clusters, culminating with combining clusters together as a reduction method for the total number of clusters that are present.\n",
    "\n",
    "Moreover, the premise for combining clusters together is a direct result of:\n",
    "\n",
    "**complete linkage** - largest Euclidean distance between clusters.  \n",
    "**single linkage** - conversely, we look at the observations which are closest together (proximity).  \n",
    "**centroid linkage** - we can the distance between the centroid of each cluster.    \n",
    "**group average (mean) linkage** - taking the mean between the pairwise distances of the observations.\n",
    "\n",
    "Complete linkage is the most traditional approach, so we parse in the `method='complete'` hyperparameter.\n",
    "\n",
    "The tree structure that examines this hierarchical structure is called a dendogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))  \n",
    "dend = shc.dendrogram(shc.linkage(mtcars, method='complete'),\n",
    "                                  labels=list(mtcars.index))\n",
    "plt.title(\"Cluster Dendrogram\"); plt.xlabel('Hclust(\"complete\")')\n",
    "plt.ylabel('Height');plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources**\n",
    "\n",
    "finnstats. (2021, October 31). What Does Cross Validation Mean? R-bloggers.  \n",
    "<a href=\"https://www.r-bloggers.com/2021/10/cross-validation-in-r-with-example/\" style=\"text-decoration:none\">https://www.r-bloggers.com/2021/10/cross-validation-in-r-with-example/</a>\n",
    "\n",
    "[](https://www.r-bloggers.com/2021/10/cross-validation-in-r-with-example/)\n",
    "\n",
    "Garbade, Michael. (2018, September 12). Understanding K-means Clustering in  \n",
    "Machine Learning. Towards Data Science.  \n",
    "<a href=\"https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1 \" style=\"text-decoration:none\">https://towardsdatascience.com/understanding-k-means-clustering-in-machine  \n",
    "-learning-6a6e67336aa1 </a>\n",
    "\n",
    "GeeksforGeeks. (2020, April 22). Scope of Variable in R. GeeksforGeeks.  \n",
    "<a href=\"https://www.geeksforgeeks.org/scope-of-variable-in-r/\" style=\"text-decoration:none\">https://www.geeksforgeeks.org/scope-of-variable-in-r/</a>\n",
    "\n",
    "Shmueli, G., Bruce, P. C., Gedeck, P., & Patel, N. R. (2020). *Data mining for  \n",
    "business analytics: Concepts, techniques and applications in Python.* Wiley.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
